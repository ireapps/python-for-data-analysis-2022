{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a65771",
   "metadata": {},
   "source": [
    "# 1. Reading data into pandas\n",
    "\n",
    "Before you can use pandas to analyze some data, you need some data. This might be a file that lives on your computer, a file that lives on the Internet or a collection of data derived from another step in your processing pipeline.\n",
    "\n",
    "There are several ways you can read data into a pandas dataframe, and you can load many different types of data files, including CSVs and other delimited text files, Excel files [and more](https://www.cbtnuggets.com/blog/2018/10/14-file-types-you-can-import-into-pandas/).\n",
    "\n",
    "Here are a few of the more common approaches.\n",
    "\n",
    "First, let's import pandas `as` pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7527838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cd5af",
   "metadata": {},
   "source": [
    "### From a CSV file\n",
    "\n",
    "If your data file is delimited with something other than a comma, you'll need to specify that in the `sep` argument. For example, if you had a pipe-delimited file: `pd.read_csv('../data/my-pipe-delimited-file.txt', sep='|')`\n",
    "\n",
    "Let's read in the MLB salary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9eeeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('../data/mlb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc25988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdd897",
   "metadata": {},
   "source": [
    "### From a CSV file on the Internet\n",
    "\n",
    "Just pass in the URL. This example uses [licensed child care facility data from Colorado's open data portal](https://data.colorado.gov/Early-childhood/Colorado-Licensed-Child-Care-Facilities-Report/a9rr-k8mu).\n",
    "\n",
    "The values that get returned aren't live -- like, if the results changed, your data frame would not update with new values. It reads in the data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_internet = pd.read_csv('https://data.colorado.gov/api/views/a9rr-k8mu/rows.csv?accessType=DOWNLOAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_internet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a791a",
   "metadata": {},
   "source": [
    "### From an Excel file\n",
    "\n",
    "To read an Excel file in pandas, use the [`read_excel()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html) method. Depending on the filetype (`xls` or `xlsx`), you'd also need to separately install into your virtual environment the `xlrd` or `openpyxl` modules. (We've already installed both here.)\n",
    "\n",
    "You might also want to specify the `sheet_name` to select your worksheet of interest -- the default is \"the first one.\"\n",
    "\n",
    "Here, we're reading in a spreadsheet with data on accidental drug overdoses in Connecticut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d9cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xl = pd.read_excel('../data/CT_Overdoses_2012-2016.xlsx', sheet_name='Accidental_Drug_Related_Deaths_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e492aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f906370",
   "metadata": {},
   "source": [
    "### From a Python data collection\n",
    "\n",
    "Maybe the work you're doing in pandas happens downstream of some other Python processing, so the data exists as a native Python data collection -- say, a list of dictionaries. You can turn this (and other Python data collections, like a list of lists) into a pandas dataframe, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {'name': 'Cody Winchester', 'job': 'Director of technology', 'location': 'Spearfish, SD'},\n",
    "    {'name': 'Guy Fieri', 'job': 'Gourmand', 'location': 'Flavortown'},\n",
    "    {'name': 'Michael Bennet', 'job': 'Senator', 'location': 'Washington, D.C.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7883af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py_lod = pd.DataFrame(data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py_lod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd8358",
   "metadata": {},
   "source": [
    "If you have a list of lists, you would need to also specify the `columns` keyword argument, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca84589",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ls = [\n",
    "    ['Cody Winchester', 'Director of technology', 'Spearfish, SD'],\n",
    "    ['Guy Fieri', 'Gourmand', 'Flavortown'],\n",
    "    ['Michael Bennet', 'Senator', 'Washington, D.C']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py_lol = pd.DataFrame(data=test_data_ls, columns=['name', 'job', 'location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b247fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py_lol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159a7c6",
   "metadata": {},
   "source": [
    "### From an HTML table\n",
    "\n",
    "OK SO.\n",
    "\n",
    "This one requires you to install and specify the Python package that has the HTML parsing engine of your choice -- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) or [lxml](http://lxml.de/). The default is `lxml`, but here we're going to use BeautifulSoup.\n",
    "\n",
    "Huge caveat! Pulling data directly from an HTML table can be hit and miss, depending on how hairy the underlying HTML is. And if you want to scrape data from a website, it's usually better practice to save the results to a local file, _then_ load it up for analysis. But it's good to know that it's an option.\n",
    "\n",
    "In this example, we've installed `BeautifulSoup` (alias `bs4`) and we're going to import [a table of lead burn instructors](https://www.texasagriculture.gov/Portals/0/Reports/PIR/certified_lead_burn_instructors.html) from the Texas Department of Agriculture website.\n",
    "\n",
    "We're going to pass three things to [the pandas `read_html()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html):\n",
    "1. The URL we want to scrape (in quotes, as a string)\n",
    "2. The `flavor` of parser that we'd like to use to process the HTML (`bs4`)\n",
    "3. The number of the list, in the list of rows that gets returned in a dataframe, that is the `header`? (Usually it's 0 -- the first one)\n",
    "\n",
    "Reading through the documentation for this method, we also notice that this method returns a _list_ of matching tables as dataframes, so we need to grab the _first_ item in this list of tables returned. Our arguments were specific enough that there's only one item in the returned list, though, so we can just grab the first item with `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a326a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_df = pd.read_html('https://www.texasagriculture.gov/Portals/0/Reports/PIR/certified_lead_burn_instructors.html',\n",
    "                       flavor='bs4',\n",
    "                       header=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a963d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c70de",
   "metadata": {},
   "source": [
    "### From a folder of identically formatted CSVs\n",
    "\n",
    "Sometimes, rather than one file you need to load, you have a directory of files with the same formatting but different data. Let's talk about a strategy for reading them all into a single dataframe -- the data for this exercise comes from [this wonderful data-driven story from 2019 by C.K. Hickey in _Foreign Policy_](https://foreignpolicy.com/all-the-presidents-meals-state-dinners-white-house-infographic/) on state dinner menus for U.S. presidents (thank you, C.K.!) and can be found in the `../data/state-dinners/` directory.\n",
    "\n",
    "Our strategy:\n",
    "- Get a list of these files using [the `glob` module](https://docs.python.org/3/library/glob.html) from the standard library\n",
    "- Use a fun Python data structure called a [\"list comprehension\"](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) in conjunction with the pandas methods `read_csv()` (which we've seen before) and [`concat()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) (which we have not)\n",
    "\n",
    "First, we need to import `glob` before we can use it. (n.b.: The customary thing to do is drop all your imports at the top of your script.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-referral",
   "metadata": {},
   "source": [
    "Get a list of the files using wildcards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_files = glob.glob('../data/state-dinners/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-sigma",
   "metadata": {},
   "source": [
    "In human language: Go to the `glob` module we just imported and use its `glob` object to get a list of files based on the path and filename wildcards we hand it.\n",
    "\n",
    "Now let's talk for a sec about **list comprehensions**. Let's say you had a list of items that you wanted to _do_ something to -- some math, some filtering, some reading into dataframes, whatever. One of the main uses for list comprehensions is effeciently \"saving\" the results of this operation to a new variable.\n",
    "\n",
    "Here's a simple example -- let's say we had the following list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-recording",
   "metadata": {},
   "source": [
    "... and we want to end up with a list of numbers that is each of these numbers multiplied by 10. We could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for x in number_list:\n",
    "    new_list.append(x*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-order",
   "metadata": {},
   "source": [
    "You could achieve the same thing with a _list comprehension_ much quicker and easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_lc = [x*10 for x in number_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-religious",
   "metadata": {},
   "source": [
    "Here, `x` is a placeholder for each item in the list, same as the variable defined in the `for` loop.\n",
    "\n",
    "That's basically what we're going to do here -- instead of creating an empty list, looping over each file in the `state_dinners` directory, creating a new dataframe, adding it to the list, then concatenating all those dataframes, we can do it all in one fell swoop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dinners = pd.concat([pd.read_csv(x) for x in sd_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-custody",
   "metadata": {},
   "source": [
    "Reading this from the inside out as a human sentence: Take each CSV file in the `state_dinners` directory, which we found earlier using the `glob` tool, and read it into a (more or less temporary) dataframe -- then take all of those dataframes and concatenate them together into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dinners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c099a15",
   "metadata": {},
   "source": [
    "## Inspecting your data\n",
    "\n",
    "Once you have your data in a data frame, your first order of business is to answer some basic questions about the data itself -- things you might want to put in your data diary, such as:\n",
    "- What's the shape of the data? (How many rows, how many columns?)\n",
    "- How many blank/null values are there in each column?\n",
    "- Did each column import as the correct type of data? (Text, number, etc.)\n",
    "- Are there any duplicate rows?\n",
    "- What are the most common values in each column? (The Golden Query™️)\n",
    "\n",
    "Let's take the Colorado child care data as an example (we read this in as `df_csv_internet` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea862c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look\n",
    "df_csv_internet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or look at the last records\n",
    "df_csv_internet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column names\n",
    "df_csv_internet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb902c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "df_csv_internet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60729ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows, how many columns?\n",
    "df_csv_internet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access each of the numbers in the .shape attribute\n",
    "no_rows = df_csv_internet.shape[0]\n",
    "no_cols = df_csv_internet.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4adcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a33a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97dfed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, use len() to check row count\n",
    "len(df_csv_internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic stats of numeric columns\n",
    "df_csv_internet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run .value_counts() against individual columns to grab most common values\n",
    "df_csv_internet.CITY.value_counts()\n",
    "# df_csv_internet.STATE.value_counts()\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d1028",
   "metadata": {},
   "source": [
    "... and so on. Another good integrity check is looking at the ranges for numeric/date columns to see if they make sense. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72533c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_internet['EXPIRATION DATE'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ad032",
   "metadata": {},
   "source": [
    "Oops! The dates in this column were imported as `object`, which (not a strictly correct definition) is a data type roughly analogous to plain text. So, in order to figure out the date range -- or to do anything with these dates -- we need to convert these values into dates.\n",
    "\n",
    "We could either go back up to where we imported this data (using the [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) method) and add a `parse_dates` argument, or we can convert the values in our current data frame using the [`to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) method.\n",
    "\n",
    "⚠️ If you didn't know how to do this already, what would be your Google?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_internet['EXPIRATION DATE'] = pd.to_datetime(\n",
    "    df_csv_internet['EXPIRATION DATE'], \n",
    "    errors='coerce',\n",
    "    infer_datetime_format=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_internet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest date\n",
    "df_csv_internet['EXPIRATION DATE'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67a55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earliest date\n",
    "df_csv_internet['EXPIRATION DATE'].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
